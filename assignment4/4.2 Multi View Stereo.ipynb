{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fcd56247c5cdf3102f1e7a9f148625a",
     "grade": false,
     "grade_id": "cell-9db57c82d6ddbb3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from numba import jit, njit\n",
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.signal\n",
    "import scipy.ndimage\n",
    "import progressbar\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91f5f25a25505730ed6bf2008e8721cc",
     "grade": false,
     "grade_id": "cell-d99a945cc738f05d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3D Reconstruction\n",
    "This week we will look at generating a virtual representation from a set of (real world) images. Given a collection of images from different angles the aim is to create a 3D reconstruction of the scene that the images capture. Common use cases include augmented reality, [virtual cameras](https://www.youtube.com/watch?v=J7xIBoPr83A) and [photogrammetry](https://www.youtube.com/watch?v=wnt64H-Wouk).\n",
    "\n",
    "## Pipeline\n",
    "Estimating the camera matrix is the first step of 3D reconstruction. As you learned in the previous notebook we can guess the camera matrix if we have a set of known pixels/3D point pairs. If we have two images of the same object then it is also possible to compute the camera matrices of each of the images by finding matching pixels. Two pixels match if they both see the same point (in 3D). We usually find these points by matching easily recognizable parts of the image such as corners, which we can find with a corner detection algorithm as discussed in the first notebook.\n",
    "\n",
    "### Setup\n",
    "For all the following assignments we will use the data set that is made publicly available by [École polytechnique fédérale de Lausanne](https://www.epfl.ch/labs/cvlab/data/data-strechamvs/). This data set already includes the camera matrices as well as a set the set of 3D points that were generated from matching pixel pairs. The following code loads the 2 input image, their camera matrices and the 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc194f5218690eacc2842d04a6804623",
     "grade": false,
     "grade_id": "cell-5671cffec4eca057",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# File with helper functions specific to this notebook\n",
    "import mvs\n",
    "\n",
    "# Alternative data set 1\n",
    "if False:\n",
    "    flip_axis = False\n",
    "    dataset_source = \"epfl\"\n",
    "    load_point_kwargs = {}\n",
    "    image0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera1\", \"001\", \"image_0000.pnm.ppm\")\n",
    "    image1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera2\", \"001\", \"image_0000.pnm.ppm\")\n",
    "    camera0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera1\", \"camera.txt\")\n",
    "    camera1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera2\", \"camera.txt\")\n",
    "    points0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera1\", \"001\", \"image_0000.pnm.ppm.featureCoord\")\n",
    "    points1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"face\", \"camera2\", \"001\", \"image_0000.pnm.ppm.featureCoord\")\n",
    "\n",
    "if True:\n",
    "    flip_axis = True\n",
    "    dataset_source = \"epfl\"\n",
    "    load_point_kwargs = {}\n",
    "    image0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"brussel\", \"rdimage.000.ppm\")\n",
    "    image1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"brussel\", \"rdimage.001.ppm\")\n",
    "    points0_file = f\"{image0_file}.3Dpoints\"\n",
    "    points1_file = f\"{image1_file}.3Dpoints\"\n",
    "    camera0_file = f\"{image0_file}.camera\"\n",
    "    camera1_file = f\"{image1_file}.camera\"\n",
    "    min_dist = 5\n",
    "    max_dist = 50\n",
    "\n",
    "# Alternative data set 2\n",
    "if False:\n",
    "    flip_axis = False\n",
    "    dataset_source = \"embree\"\n",
    "    load_point_kwargs = { \"relative_number_of_points\": 0.005 }\n",
    "    image0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"color\", \"cam1.png\")\n",
    "    image1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"color\", \"cam2.png\")\n",
    "    camera0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"cameras\", \"cam1.txt\")\n",
    "    camera1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"cameras\", \"cam2.txt\")\n",
    "    points0_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"world_pos\", \"cam1.npy\")\n",
    "    points1_file = os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"sponza\", \"world_pos\", \"cam2.npy\")\n",
    "    min_dist = 100\n",
    "    max_dist = 2000\n",
    "\n",
    "image0 = helpers.imread_normalized_float_grayscale(image0_file)\n",
    "image1 = helpers.imread_normalized_float_grayscale(image1_file)\n",
    "if flip_axis:\n",
    "    image0 = image0.T[::-1,:]\n",
    "    image1 = image1.T[::-1,:]\n",
    "image0_half_res = helpers.resize_image(image0, 0.5)\n",
    "image1_half_res = helpers.resize_image(image1, 0.5)\n",
    "image0_quarter_res = helpers.resize_image(image0, 0.25)\n",
    "image1_quarter_res = helpers.resize_image(image1, 0.25)\n",
    "\n",
    "image0_color = helpers.imread_normalized_float(image0_file)\n",
    "image1_color = helpers.imread_normalized_float(image1_file)\n",
    "if flip_axis:\n",
    "    image0_color = mvs.flip_color_image_axis(image0_color)\n",
    "    image1_color = mvs.flip_color_image_axis(image1_color)\n",
    "image0_color_half_res = helpers.resize_image(image0_color, 0.5)\n",
    "image1_color_half_res = helpers.resize_image(image1_color, 0.5)\n",
    "image0_color_quarter_res = helpers.resize_image(image0_color, 0.25)\n",
    "image1_color_quarter_res = helpers.resize_image(image1_color, 0.25)\n",
    "\n",
    "# NOTE: the camera data is stored as a tuple of (position, intrinsic matrix, extrinsic matrix)\n",
    "# 3D points may be projected into 2D as follows:\n",
    "# tmp = (intr * extr) * vec4(point3D, 1)\n",
    "# pixel = tmp.xy / tmp.z\n",
    "#\n",
    "# We have implemented this function for you in mvs.py (see project_points(camera, points3D))\n",
    "#camera0 = mvs.load_camera_data(camera0_file)\n",
    "#camera1 = mvs.load_camera_data(camera1_file)\n",
    "camera0 = mvs.load_camera(camera0_file, dataset_source)\n",
    "camera1 = mvs.load_camera(camera1_file, dataset_source)\n",
    "if flip_axis:\n",
    "    camera0 = mvs.flip_camera_axis(camera0)\n",
    "    camera1 = mvs.flip_camera_axis(camera1)\n",
    "camera0_half_res = mvs.scale_camera(camera0, 0.5)\n",
    "camera1_half_res = mvs.scale_camera(camera1, 0.5)\n",
    "camera0_quarter_res = mvs.scale_camera(camera0, 0.25)\n",
    "camera1_quarter_res = mvs.scale_camera(camera1, 0.25)\n",
    "\n",
    "points3D_0 = mvs.load_points3D(points0_file, dataset_source, **load_point_kwargs)\n",
    "points3D_1 = mvs.load_points3D(points1_file, dataset_source, **load_point_kwargs)\n",
    "\n",
    "def plot_image_and_points(ax, image, camera_data, points3D):\n",
    "    points2D = mvs.project_points(camera_data, points3D)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image)\n",
    "    # Show up to 250 points so that we can still see the image underneath.\n",
    "    ax.scatter(points2D[:250,0], points2D[:250,1])\n",
    "    \n",
    "fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "plot_image_and_points(ax0, image0_color, camera0, points3D_0)\n",
    "plot_image_and_points(ax1, image1_color, camera1, points3D_0)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dd29c8ecc0e336bbf7b21af2b2815a6",
     "grade": false,
     "grade_id": "cell-583d2aa82725bf58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Plotting the known 3D points\n",
    "The following code plots the 3D points that were generated when computing the camera matrices. If you move the camera around you should start to recognize the general shape of the building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2e109d3ddfbb53589b102b63775d81",
     "grade": false,
     "grade_id": "cell-cb98d291bd0d1073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# View it.\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode()\n",
    "\n",
    "def plotly_points(points, size=2, color=None, label=\"Points\"):\n",
    "    # 3D plot\n",
    "    if flip_axis:\n",
    "        x = points[:, 1]\n",
    "        y = points[:, 2]\n",
    "        z = points[:, 0]\n",
    "    else:\n",
    "        # In graphics z = depth, when plotting z = height\n",
    "        x = points[:, 0]\n",
    "        z = points[:, 1]\n",
    "        y = points[:, 2]\n",
    "    \n",
    "    if color is None:\n",
    "         color = np.random.rand(len(points))\n",
    "    elif isinstance(color, list) or isinstance(color, np.ndarray):\n",
    "        color = [f\"rgb({255*r},{255*g},{255*b})\" for b, g, r in color]\n",
    "        \n",
    "    return go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=size,\n",
    "            color=color, # set color to an array/list of desired values\n",
    "            colorscale='Viridis', # choose a colorscale\n",
    "            opacity=1.0\n",
    "        ),\n",
    "        name=label\n",
    "    )\n",
    "    \n",
    "def plotly_point(point, **kwargs):\n",
    "    return plotly_points(np.array([point]), **kwargs)\n",
    "\n",
    "def plotly_camera(camera, size=8, label=\"Camera\", **kwargs):\n",
    "    return plotly_point(camera[0], size=size, label=label, **kwargs)\n",
    "\n",
    "def plotly_frustum(camera, resolution):\n",
    "    h, w = resolution\n",
    "    p0 = camera[0]\n",
    "    _, d0 = mvs.pixel_to_ray(camera, (0, 0))\n",
    "    _, d1 = mvs.pixel_to_ray(camera, (w, 0))\n",
    "    _, d2 = mvs.pixel_to_ray(camera, (w, h))\n",
    "    _, d3 = mvs.pixel_to_ray(camera, (0, h))\n",
    "    \n",
    "    dist = 30\n",
    "    vertices = np.array([p0, p0 + dist*d0, p0 + dist*d1, p0 + dist*d2, p0 + dist*d3])\n",
    "    indices = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 4], [0, 4, 1]])\n",
    "    if flip_axis:\n",
    "        x = vertices[:, 1]\n",
    "        y = vertices[:, 2]\n",
    "        z = vertices[:, 0]\n",
    "    else:\n",
    "        # In graphics z = depth, when plotting z = height\n",
    "        x = vertices[:, 0]\n",
    "        z = vertices[:, 1]\n",
    "        y = vertices[:, 2]\n",
    "    return go.Mesh3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        i=indices[:,0],\n",
    "        j=indices[:,1],\n",
    "        k=indices[:,2],\n",
    "        facecolor=[\"red\", \"green\", \"blue\", \"yellow\"],\n",
    "        opacity=0.2\n",
    "    )\n",
    "    \n",
    "def plotly_epipolar_plane(camera0, camera1, points):\n",
    "    camera0_pos = camera0[0]\n",
    "    camera1_pos = camera1[0]\n",
    "    \n",
    "    dist = 5\n",
    "    vertices = np.append(np.array([camera0_pos, camera1_pos]), points, axis=0)\n",
    "    indices = np.array([[0, 1, i+2] for i in range(len(points))])\n",
    "    if flip_axis:\n",
    "        x = vertices[:, 1]\n",
    "        y = vertices[:, 2]\n",
    "        z = vertices[:, 0]\n",
    "    else:\n",
    "        # In graphics z = depth, when plotting z = height\n",
    "        x = vertices[:, 0]\n",
    "        z = vertices[:, 1]\n",
    "        y = vertices[:, 2]\n",
    "    return go.Mesh3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        i=indices[:,0],\n",
    "        j=indices[:,1],\n",
    "        k=indices[:,2],\n",
    "        facecolor=[\"red\"],\n",
    "        opacity=1.0\n",
    "    )\n",
    "\n",
    "# Project points onto image and get color from pixel\n",
    "points2D_0 = mvs.project_points(camera0, points3D_0)\n",
    "colors = []\n",
    "for x, y in points2D_0:\n",
    "    colors.append(image0_color[helpers.round_to_int(y), helpers.round_to_int(x)])\n",
    "\n",
    "trace1 = plotly_points(points3D_0, color=colors)\n",
    "trace2 = plotly_camera(camera0, size=8, color=\"red\", label=\"Camera 0\")\n",
    "trace3 = plotly_frustum(camera0, image0.shape)\n",
    "trace4 = plotly_camera(camera1, size=8, color=\"blue\", label=\"Camera 1\")\n",
    "trace5 = plotly_frustum(camera1, image0.shape)\n",
    "trace6 = plotly_epipolar_plane(camera0, camera1, points3D_0[:1])\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "layout = go.Layout(margin={\"l\": 0, \"r\": 0, \"b\": 0, \"t\": 0}, scene={\"aspectmode\": \"cube\"})\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='3d-scatter-colorscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7938c0c17354fcfe4e2c8fc628af6bb",
     "grade": false,
     "grade_id": "cell-9e53951f2152e2a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Triangulating matching view rays\n",
    "If we know that two pixels are looking at the same point in 3D then it is possible to compute the position of that 3D point based on the pixel coordinates in both images. Each pixel defines a ray in 3D. A ray is a half open light segment: it starts at the camera and it goes through the pixel towards infinity. Image the two rays corresponding to the matching pixels: one starting from `camera0` and the other at `camera1`. If the rays are defined with infinite precision then they will intersect at the 3D point that is visible at those pixels. Due to measurement and computer arithmetic errors it is very uncommon for two rays to exactly intersect however. So instead we have to look for the closest point between the two rays.\n",
    "\n",
    "A ray is defined by the following formula: $\\vec{p}=\\vec{o}+t\\vec{d}$. Here, $\\vec{o}$ is the starting point and $\\vec{d}$ is the direction of the ray. $t$ a ray with a particular length.\n",
    "\n",
    "Given two rays $\\vec{p}=\\vec{o_0}+t\\vec{d_0}$ and $\\vec{q}=\\vec{o_1}+s\\vec{d_1}$ we want to find the closest point between the two rays.\n",
    "\n",
    "The closest point lies on the line between the two points $\\vec{p}$ and $\\vec{q}$ on ray 1 and 2 respectively for which $\\lVert pq \\rVert$ is minimal. This line $pq=\\vec{q} - \\vec{p}$ is perpendicular to both rays.\n",
    "\n",
    "$$\n",
    "\\vec{d_0} \\cdot pq = 0 \\text{ and } \\vec{d_1} \\cdot pq = 0 \\\\\n",
    "\\vec{d_0} \\cdot (\\vec{o_1} + s\\vec{d_1} - \\vec{o_0} - t\\vec{d_0}) = 0 \\text{ and } \\vec{d_1} \\cdot (\\vec{o_1} + s\\vec{d_1} - \\vec{o_0} - t\\vec{d_0}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{d_0}\\cdot\\vec{d_1}s - \\vec{d_0}\\cdot\\vec{d_0}t = \\vec{o_0}\\cdot\\vec{d_0} - \\vec{o_1}\\cdot\\vec{d_0} \\text{ and } \\vec{d_1}\\cdot\\vec{d_1}s - \\vec{d_0}\\cdot\\vec{d_1}t = \\vec{o_0}\\cdot\\vec{d_1} - \\vec{o_1}\\cdot\\vec{d_1}\n",
    "$$\n",
    "\n",
    "Multiply the former by $\\vec{d_0}\\cdot\\vec{d_1}$ and the latter by $\\vec{d_0}\\cdot\\vec{d_0}$:\n",
    "$$\n",
    "\\vec{d_0}\\cdot\\vec{d_1}\\vec{d_0}\\cdot\\vec{d_1}s - \\vec{d_0}\\cdot\\vec{d_1}\\vec{d_0}\\cdot\\vec{d_0}t = \\vec{d_0}\\cdot\\vec{d_1}(\\vec{o_0}\\cdot\\vec{d_0} - \\vec{o_1}\\cdot\\vec{d_0}) \\text{, and:}\\\\\n",
    "\\vec{d_0}\\cdot\\vec{d_0}\\vec{d_1}\\cdot\\vec{d_1}s - \\vec{d_0}\\cdot\\vec{d_1}\\vec{d_0}\\cdot\\vec{d_0}t = \\vec{d_0}\\cdot\\vec{d_0}(\\vec{o_0}\\cdot\\vec{d_1} - \\vec{o_1}\\cdot\\vec{d_1})\n",
    "$$\n",
    "\n",
    "By subtracting both equations we can derive $s$:\n",
    "$$\n",
    "(\\vec{d_0}\\cdot\\vec{d_1}\\vec{d_0}\\cdot\\vec{d_1} - \\vec{d_0}\\cdot\\vec{d_0}\\vec{d_1}\\cdot\\vec{d_1})s = \\vec{d_0}\\cdot\\vec{d_1}(\\vec{o_0}\\cdot\\vec{d_0} - \\vec{o_1}\\cdot\\vec{d_0}) - \\vec{d_0}\\cdot\\vec{d_0}(\\vec{o_0}\\cdot\\vec{d_1} - \\vec{o_1}\\cdot\\vec{d_1}) \\\\\n",
    "s = \\frac{\\vec{d_0}\\cdot\\vec{d_1}(\\vec{o_0}\\cdot\\vec{d_0} - \\vec{o_1}\\cdot\\vec{d_0}) - \\vec{d_0}\\cdot\\vec{d_0}(\\vec{o_0}\\cdot\\vec{d_1} - \\vec{o_1}\\cdot\\vec{d_1})} {\\vec{d_0}\\cdot\\vec{d_1}\\vec{d_0}\\cdot\\vec{d_1} - \\vec{d_0}\\cdot\\vec{d_0}\\vec{d_1}\\cdot\\vec{d_1}}\n",
    "$$\n",
    "\n",
    "With $s$ known we can now rewrite either one of the earlier equations to compute $t$:\n",
    "$$\n",
    "\\vec{d_0}\\cdot\\vec{d_1}s - \\vec{d_0}\\cdot\\vec{d_0}t = \\vec{o_0}\\cdot\\vec{d_0} - \\vec{o_1}\\cdot\\vec{d_0} \\\\\n",
    "\\vec{d_0}\\cdot\\vec{d_0}t = \\vec{d_0}\\cdot\\vec{d_1}s - \\vec{o_0}\\cdot\\vec{d_0} + \\vec{o_1}\\cdot\\vec{d_0} \\\\\n",
    "t = \\frac{\\vec{d_0}\\cdot\\vec{d_1}s - \\vec{o_0}\\cdot\\vec{d_0} + \\vec{o_1}\\cdot\\vec{d_0}} {\\vec{d_0}\\cdot\\vec{d_0}}\n",
    "$$\n",
    "\n",
    "### Exercise 8 (2 points)\n",
    "Implement a function that finds the closest points $p$ and $q$ on both rays as described above.\n",
    "\n",
    "**NOTE** Because this function will be called many times performance is critical. In this case we cannot efficiently express ourselfs with Python/numpy alone so we will use *Numba* to translate our code into native machine instructions. Numba supports most numpy functions (such as `np.dot`) but can be pedantic regarding implicit data type conversions so please be aware. **Do not remove the `@njit` decorator from your final solution** (you are free to do so while working on the exercise). You can use `np.divide` to divide two numbers without division by zero throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c63c69cd2b69ed5695adfeb7a34e2ca4",
     "grade": false,
     "grade_id": "exercise8_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the closest points p and q on ray 0 and ray 1\n",
    "@njit\n",
    "def closest_points_on_rays(ray0, ray1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return (p, q)\n",
    "    \n",
    "# Find the closest point between both rays (half way between p and q)\n",
    "@njit\n",
    "def closest_point_between_rays(ray0, ray1):\n",
    "    p0, p1 = closest_points_on_rays(ray0, ray1)\n",
    "    return (p0 + p1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0caa85eacf5744aeae0c510cbc6b1416",
     "grade": false,
     "grade_id": "cell-f8b5c6ff55b853c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test of exercise 8\n",
    "To test your solution we will project the set of known 3D points into both images. Then we try to compute the 3D positions of each point by \"shooting\" a ray through the pixel in both images onto which the 3D point projects. The graph plots the percentage of points who's error is smaller than the value that is indicated on the horizontal axis. The error should never be larger than $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d77a094e5b3c05155b8862fc4a3ff2be",
     "grade": false,
     "grade_id": "asdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "points2D_0 = mvs.project_points(camera0, points3D_0)\n",
    "points2D_1 = mvs.project_points(camera1, points3D_0)\n",
    "\n",
    "triangulation_errors = np.empty((len(points3D_0)), np.float32)\n",
    "for i, point3D_ref in enumerate(points3D_0):\n",
    "    pixel0 = points2D_0[i,:].astype(int)\n",
    "    pixel1 = points2D_1[i,:].astype(int)\n",
    "    \n",
    "    ray0 = mvs.pixel_to_ray(camera0, pixel0)\n",
    "    ray1 = mvs.pixel_to_ray(camera1, pixel1)\n",
    "    \n",
    "    point3D = closest_point_between_rays(ray0, ray1)\n",
    "    triangulation_errors[i] = np.linalg.norm(point3D - point3D_ref)\n",
    "\n",
    "x = np.linspace(0, 100, 100)\n",
    "y = np.percentile(triangulation_errors, x)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=helpers.default_fig_size)\n",
    "ax.set_title(\"Distance (error) between position estimated by your solution and actual position\")\n",
    "ax.set_xlabel(\"Error\")\n",
    "ax.set_ylabel(\"Percentile of points\")\n",
    "ax.plot(y, x)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "assert(np.mean(triangulation_errors) < 0.1)\n",
    "assert(np.max(triangulation_errors) < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66a5c0570f843a2f0d72939ff4f3a920",
     "grade": true,
     "grade_id": "exercise8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a72b39e7c6d2c05df0711969772a09a8",
     "grade": false,
     "grade_id": "cell-62f7a9c4f3d2358c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Epipolar lines\n",
    "To create a 3D reconstruction from two images we have to find matching pixel pairs. Comparing each pixel in one image with each pixel in the other image is not only inefficient but might also lead to matches who's rays do not get close to each other. We could enforce this by discarding pixels whos rays do not get near eachother.\n",
    "\n",
    "The demo below allows you to interactively visualize which pixels from the right image might see a point in the left image (their rays are very close). **Click anywhere on the left image to pick a point**. For each pixel in the right image we \"shoot\" a ray and if that ray (almost) intersects the ray from the first camera (going through the clicked pixel) then the pixel is marked (assigned a color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f7bce3c9045834ec22c1e5c1e7b931",
     "grade": false,
     "grade_id": "cell-c0d75fc433c17a85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def draw_epipolar_line(image1, camera0, camera1, pixel0, color):\n",
    "    out_image = image1.copy()\n",
    "    \n",
    "    ray0 = mvs.pixel_to_ray(camera0, pixel0)\n",
    "    height, width, _ = image1.shape\n",
    "    for y1 in range(height):\n",
    "        for x1 in range(width):\n",
    "            pixel1 = (x1, y1)\n",
    "            ray1 = mvs.pixel_to_ray(camera1, pixel1)\n",
    "\n",
    "            p0, p1 = closest_points_on_rays(ray0, ray1)\n",
    "            p0 = mvs.project_point(camera1, p0)\n",
    "            p1 = mvs.project_point(camera1, p1)\n",
    "            if np.linalg.norm(p0 - p1) < 1:\n",
    "                out_image[y1, x1] = color\n",
    "    return out_image\n",
    "\n",
    "def plot_interactive_epipolar_lines(image0, image1, camera0, camera1):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    clicked_points = []\n",
    "    output_image = image1.copy()\n",
    "    \n",
    "    def redraw():\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.axis(\"off\")\n",
    "        ax2.axis(\"off\")\n",
    "        ax1.imshow(image0)\n",
    "        for pixel, color in clicked_points:\n",
    "            ax1.scatter([pixel[0]], [pixel[1]], c=[color])\n",
    "        ax2.imshow(output_image)\n",
    "    \n",
    "    def onclick(event):\n",
    "        nonlocal output_image\n",
    "                \n",
    "        color = np.random.rand((3)).astype(np.float32)\n",
    "        pixel = (event.xdata, event.ydata)\n",
    "        \n",
    "        clicked_points.append((pixel, color))\n",
    "        output_image = draw_epipolar_line(output_image, camera0, camera1, pixel, color)\n",
    "        redraw()\n",
    "    \n",
    "    # Call draw_epipolar_line (discarding output) so that it is compiled before the user starts clicking.\n",
    "    print(\"Compiling...\")\n",
    "    draw_epipolar_line(output_image, camera0, camera1, (100, 100), np.ones((3), np.float32))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    redraw()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", onclick)\n",
    "    #fig.show()\n",
    "\n",
    "#image0 = imread_normalized_float_grayscale(image0_file).T[::-1,:]\n",
    "#image1 = imread_normalized_float_grayscale(image1_file).T[::-1,:]\n",
    "\n",
    "plot_interactive_epipolar_lines(image0_color_quarter_res, image1_color_quarter_res, camera0_quarter_res, camera1_quarter_res)\n",
    "# TODO: (exercise) click on image and draw epipolar line in the other image. 3D view next to it visualizing the 3D points while they are added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5198aeff316aa394a07683a6f3c1ad81",
     "grade": false,
     "grade_id": "cell-c115077690a638a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see the rays through pixels in the left image create lines in the right image. These lines are called epipolar lines. Intuitively, we are projecting a 3 dimensional line (ray) onto a the camera sensor (which is a plane) which results in a line in 2D. We can take advantage of this knowledge to speed up 3D reconstruction. Instead of comparing every pixel in the left image to every pixel in the right image we only have to compare the pixels along a line.\n",
    "\n",
    "## Computing epipolar lines more efficiently\n",
    "We can also compute and draw the epipolar line without having to go over all pixels in the image (which is very slow). The line contains all the pixels (in the right image) that (almost) intersect a ray through a pixel of the left image. So what we can do is compute two arbitrary points along this ray (in 3D) and then project those two points onto the right image. The epipolar line is then simply the *2D* line through those two pixels.\n",
    "\n",
    "The following code demonstrates how this can be done efficiently. Click on the left image to create a ray. The epipolar line will be draw in the right image including the two (arbitrary) points along the ray which were used to draw the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba9d62f10f9c442a7e0c3624a696395",
     "grade": false,
     "grade_id": "cell-1ba75a7a7a32e908",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_epipolar_line2(image1, ax2, camera0, camera1, pixel0, color):\n",
    "    # Create a ray going through pixel0 of camera0\n",
    "    ray = mvs.pixel_to_ray(camera0, pixel0)\n",
    "    origin, direction = ray\n",
    "    \n",
    "    # Take two arbitrary points on the ray and transform them into the second image\n",
    "    p0 = mvs.project_point(camera1, origin + 10 * direction)\n",
    "    p1 = mvs.project_point(camera1, origin + 40 * direction)\n",
    "\n",
    "    # Draw the two points that we used in the right image\n",
    "    ax2.scatter([p0[0], p1[0]], [p0[1], p1[1]], c=color.reshape(1,3))\n",
    "    \n",
    "    # Draw a line through the points in the right image\n",
    "    return mvs.draw_line_through_pixels(image1, p0, p1, color)\n",
    "\n",
    "\n",
    "def plot_interactive_epipolar_lines(image0, image1, camera0, camera1):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    clicked_points = []\n",
    "    output_image = image1.copy()\n",
    "    \n",
    "    def clear():\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.axis(\"off\")\n",
    "        ax2.axis(\"off\")\n",
    "    \n",
    "    def redraw():\n",
    "        ax1.imshow(image0)\n",
    "        for pixel, color in clicked_points:\n",
    "            ax1.scatter([pixel[0]], [pixel[1]], c=color.reshape(1,3))\n",
    "        ax2.imshow(output_image)\n",
    "    \n",
    "    def onclick(event):\n",
    "        nonlocal output_image\n",
    "                \n",
    "        color = np.random.rand((3)).astype(np.float32)\n",
    "        pixel = (event.xdata, event.ydata)\n",
    "        \n",
    "        clicked_points.append((pixel, color))\n",
    "        clear()\n",
    "        output_image = draw_epipolar_line2(output_image, ax2, camera0, camera1, pixel, color)\n",
    "        redraw()\n",
    "    \n",
    "    # Call draw_epipolar_line (discarding output) so that it is compiled before the user starts clicking.\n",
    "    print(\"Compiling...\")\n",
    "    draw_epipolar_line2(output_image, ax2, camera0, camera1, (100, 100), np.ones(3))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    clear()\n",
    "    redraw()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", onclick)\n",
    "    #fig.show()\n",
    "\n",
    "plot_interactive_epipolar_lines(image0_color_quarter_res, image1_color_quarter_res, camera0_quarter_res, camera1_quarter_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e86b0d90427e831d88fb37d7c6013cd",
     "grade": false,
     "grade_id": "cell-bff127e77df0d9d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Photo consistency measures\n",
    "To find which pixels are seeing the same 3D point, we need a way of expressing the similarity between two pixels (in two different images). A single pixel (3 values in RGB) often does not capture enough information to find the correct match out of thousands of potential matches. So we define a support domain, which is a small region around around the pixel of interest (similar to the cutout in the corner detection exercises).\n",
    "\n",
    "A photo consistency measure is a function that compares two support domains and returns a number which indicates how similar the two regions are. In the past assignments we have often used the Sum of Squared Differences (SSD) measure to compare how far your solutions were from the reference output. The sum of squared differences can also be used as a photo consistency measure. It's formula is as follows: $\\sum\\limits_i^N (x_i - y_i)^2$. The SSD can be susceptible to large variations within one pixel. To combat this issue we can also use the Sum of Absolute Differences (SAD) which is defined as $\\sum\\limits_i^N \\lVert x_i - y_i \\rVert / N$. Like SSD, the sum of absolute differences assigns a lower score to images with a high similarity. Because we want our photo consistency function to do the opposite we should return the negated ($-SSD$ or $-SAD$) score.\n",
    "\n",
    "Finally, a slightly more advanced photo consistency measure is the Normalised Cross Correlation (NCC). Instead of considering absolute pixel values, NCC compares the differences relative to the mean intensity (over the support domain). Support domains with a high standard deviation will result in a higher score since they are more detailed and thus more unique. The NCC measure is defined as:\n",
    "\n",
    "$$\n",
    "norm\\_corr(x,y)=\\dfrac{\\sum\\limits_i (x_i - \\overline{x}) (y_i - \\overline{y}) }{\\sqrt{\\sum\\limits_i (x_i - \\overline{x})^2 \\sum\\limits_i (y - \\overline{y})^2}}\n",
    "$$\n",
    "\n",
    "### Exercise 9 (2 points)\n",
    "Implement the SAD, SSD and NCC photo consistency measures. The functions take two equally sized (2D) arrays containing the pixel values in the support domain. Make sure that all functions return higher values for higher similarities.\n",
    "\n",
    "To test your solution we take one of the known 3D points and compute the photo consistency along its epipolar line in one of the images. The vertical red line indicates which pixel actually matches and thus should have the highest photo consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bf965c0ca1f757ccaa3bb85da4de9fa",
     "grade": false,
     "grade_id": "exercise9_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Asume that pixel0 and pixel1 are inside the image but the support domain might not be\n",
    "@njit\n",
    "def photo_consistency_SSD(support0, support1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "@njit\n",
    "def photo_consistency_SAD(support0, support1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "@njit\n",
    "def photo_consistency_NCC(support0, support1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "# === VISUALIZATION SETTINGS ===\n",
    "point3D = points3D_0[123,:]\n",
    "pc_func = photo_consistency_SSD\n",
    "support_domain = 16\n",
    "\n",
    "mvs.plot_photoconsistency_along_ray(image0, image1, camera0, camera1, point3D, pc_func, support_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52f15d779b6f05fe793838fedfbba950",
     "grade": false,
     "grade_id": "cell-8f2df16325d4946d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test of exercise 9\n",
    "The graph in the cell above will show you the response of a photo consistency function along a ray. The pixel with the highest photo consistency value will be matched. The actual matching pixel (for the set of known 3D points) is illustrated with a red vertical line. Try out different photo consistency functions and support domain sizes for different points. The photo consistency will not always be correct but it should be a good estimate in most cases, see the paragraph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "760abddedaa955a5feed54e3aa7f2c1f",
     "grade": true,
     "grade_id": "exercise9_correct",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4fd2593d902d50c6875a6ebfeaa76fd",
     "grade": false,
     "grade_id": "cell-d3d0e07b12133c67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Photo consistency accuracy\n",
    "To get a better idea of how the photo consistency measures perform we can repeat the previous example for each of the known 3D points. The photo consistency measure should peak at the pixel where the 3D point is located in the image. The following code plots the percentile (cumulative percentage) of points whose photo consistency peak lies within a certain distance of the correct pixel. Note that a photo consistency measure is not perfect: not all pixels will match to the correct points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0600afd826c0433febf46dd94104b14a",
     "grade": false,
     "grade_id": "cell-3dc0916b9f2b54a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Scale image & camera to improve performance\n",
    "points2D_cam0 = mvs.project_points(camera0, points3D_0)\n",
    "points2D_cam1 = mvs.project_points(camera1, points3D_0)\n",
    "\n",
    "# === TEST SETTINGS ===\n",
    "photo_consistency_func = photo_consistency_SSD\n",
    "support_domain = 4\n",
    "\n",
    "errors_in_pixels = []\n",
    "for pixel0, pixel1 in progressbar.progressbar(list(zip(points2D_cam0.astype(np.int), points2D_cam1.astype(np.int)))):\n",
    "    height, width = image0.shape\n",
    "    x, y = pixel0\n",
    "    if x < support_domain or x >= width - support_domain or y < support_domain or y >= height - support_domain:\n",
    "        continue    # Walk along the ray and compute the photo consistency at each pixel\n",
    "    \n",
    "    pixels, photo_consistency = mvs.compute_photo_consistency_along_ray(\n",
    "        image0, image1, camera0, camera1, pixel0, photo_consistency_func, support_domain)\n",
    "    \n",
    "    if len(pixels) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute where along the ray the point should actually be\n",
    "    pixel1_pos_along_ray = np.argmin(np.sum((pixels - pixel1)**2, axis=1)) # Closest pixel = minimize SSD\n",
    "    best_pixel_index = np.argmax(photo_consistency)\n",
    "    errors_in_pixels.append(abs(pixel1_pos_along_ray - best_pixel_index))\n",
    "\n",
    "print(\"Number of correct matches (<3): \", np.count_nonzero(np.array(errors_in_pixels) < 3), \" out of \", len(errors_in_pixels))\n",
    "\n",
    "x = np.linspace(0, 100, 100)\n",
    "y = np.percentile(errors_in_pixels, x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=helpers.default_fig_size)\n",
    "ax.set_title(\"Distance between highest photo consistency pixel and correct pixel\")\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel(\"Percentile\")\n",
    "ax.set_ylabel(\"Distance\")\n",
    "ax.xaxis.set_major_formatter(StrMethodFormatter('{x:.0f}%'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55c217017f57cfdc54ac74fb636251a9",
     "grade": false,
     "grade_id": "cell-59b5e6f279fb5702",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Interactive 3D reconstruction\n",
    "We now have all ingredients to perform 3D reconstruction. We can efficiently iterate over the pixels on the epipolar line and we can find a match among the remaining pixels using a photo consistency measure. Finally, when we have two matching pixels we can compute the 3D point where the rays through those two pixels (almost) intersect.\n",
    "\n",
    "The following live demo shows everything in action. **Click anywhere in the left image** and the program will automatically compute and draw the photo consistency along the ray (of the pixel you clicked) in the right image. It will also highlight the pixel with the highest photo consistency measure. A ray is shot through the highlighted pixel and a 3D point is computed by intersecting it's ray with the ray through the pixel in the left image.\n",
    "\n",
    "**NOTE** The set of known 3D points (as described at the start of the notebook) are displayed as black. Points found with this new method are added in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79c99392ad3fdeff85e227954ef4a7b5",
     "grade": false,
     "grade_id": "cell-24b88c06c28468d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Computes the photo consistency along the epipolar line in image1 of the ray through pixel0 (of camera0).\n",
    "# The epipolar line is colored using gray scale according the photo consistency measure of each pixel.\n",
    "#@njit\n",
    "def draw_epipolar_photo_consistency(image0, image1, camera0, camera1, pixel0, pc_func, support_domain):\n",
    "    out_image = image1.copy()\n",
    "    \n",
    "    pixel1s, photo_consistencies = mvs.compute_photo_consistency_along_ray_color_avg(image0, image1, camera0, camera1, pixel0, pc_func, support_domain)\n",
    "    min_pc = np.min(photo_consistencies)\n",
    "    max_pc = np.max(photo_consistencies)\n",
    "    for (x1, y1), photo_consistency in zip(pixel1s, photo_consistencies):\n",
    "        photo_consistency = ((photo_consistency - min_pc) / (max_pc - min_pc)) ** 4\n",
    "        # Draw a slightly thicker (vertically) line\n",
    "        out_image[y1-1, x1] = photo_consistency\n",
    "        out_image[y1+0, x1] = photo_consistency\n",
    "        out_image[y1+1, x1] = photo_consistency\n",
    "    \n",
    "    # Return the pixel with the highest photo consistency\n",
    "    return out_image, tuple(pixel1s[np.argmax(photo_consistencies)])\n",
    "\n",
    "\n",
    "# Complex 3D plotting stuff = you do not need to understand this\n",
    "def plot_scatter3D(event_queue):\n",
    "    fig = go.FigureWidget(layout={\"scene\": {\"aspectmode\": \"cube\"}})\n",
    "    if flip_axis:\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=points3D_0[:,1],\n",
    "                y=points3D_0[:,2],\n",
    "                z=points3D_0[:,0],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=3,\n",
    "                    color=\"rgb(50,50,50)\", # set color to an array/list of desired values\n",
    "                    colorscale='Viridis', # choose a colorscale\n",
    "                    opacity=0.5\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=points3D_0[:,0],\n",
    "                y=points3D_0[:,1],\n",
    "                z=points3D_0[:,2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=3,\n",
    "                    color=\"rgb(50,50,50)\", # set color to an array/list of desired values\n",
    "                    colorscale='Viridis', # choose a colorscale\n",
    "                    opacity=0.5\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[],\n",
    "            y=[],\n",
    "            z=[],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=[], # set color to an array/list of desired values\n",
    "                colorscale='Viridis', # choose a colorscale\n",
    "                opacity=0.5\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces()\n",
    "\n",
    "    def event_handler(event_queue):\n",
    "        while True:\n",
    "            point3D, color = event_queue.get() # With timeout so it is possible to interupt the thread\n",
    "            scatter = fig.data[1]\n",
    "            x =  list(scatter[\"x\"])\n",
    "            y =  list(scatter[\"y\"])\n",
    "            z =  list(scatter[\"z\"])\n",
    "            colors = list(scatter[\"marker\"][\"color\"])\n",
    "            \n",
    "            if flip_axis:\n",
    "                x.append(point3D[1])\n",
    "                y.append(point3D[2])\n",
    "                z.append(point3D[0])\n",
    "            else:\n",
    "                x.append(point3D[0])\n",
    "                y.append(point3D[1])\n",
    "                z.append(point3D[2])\n",
    "                \n",
    "            r, g, b = color\n",
    "            colors.append(f\"rgb({255*r}, {255*g}, {255*b})\")\n",
    "            \n",
    "            with fig.batch_update():\n",
    "                scatter[\"x\"] = x\n",
    "                scatter[\"y\"] = y\n",
    "                scatter[\"z\"] = z\n",
    "                scatter[\"marker\"][\"color\"] = colors\n",
    "    \n",
    "    t = threading.Thread(target=event_handler, args=(event_queue,), daemon=True)\n",
    "    t.start() # WARNING: thread is not killed when cell is restarted (so this is leaking memory)\n",
    "\n",
    "    scatter = fig.data[0]\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_epipolar_line_images2D(event_queue, image0, image1, camera0, camera1, pc_func, support_domain, min_dist, max_dist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    output_image = image1.copy()\n",
    "    \n",
    "    def redraw(pixel0=None, pixel1=None):\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.axis(\"off\")\n",
    "        ax2.axis(\"off\")\n",
    "        ax1.imshow(image0)\n",
    "        ax2.imshow(output_image)\n",
    "        if pixel0:\n",
    "            ax1.scatter([pixel0[0]], [pixel0[1]], c=\"r\")\n",
    "        if pixel1:\n",
    "            ax2.scatter([pixel1[0]], [pixel1[1]], c=\"r\")\n",
    "    \n",
    "    def onclick(event):\n",
    "        # === User clicked on the image ===\n",
    "        nonlocal output_image, event_queue\n",
    "        pixel0 = (int(event.xdata), int(event.ydata))\n",
    "        \n",
    "        # Compute the epipolar line and draw it onto image 1\n",
    "        output_image, pixel1 = draw_epipolar_photo_consistency(image0, image1, camera0, camera1, pixel0, pc_func, support_domain)\n",
    "        \n",
    "        # Shoot a ray through the clicked pixel and the matching pixel (with the highest photo consistency)\n",
    "        ray0 = mvs.pixel_to_ray(camera0, pixel0)\n",
    "        ray1 = mvs.pixel_to_ray(camera1, pixel1)\n",
    "        # Find closest point between the two rays\n",
    "        point3D = closest_point_between_rays(ray0, ray1)\n",
    "        \n",
    "        # Ignore matches which are clearly wrong (triangulated point extremely close/far from the camera)\n",
    "        dist = np.linalg.norm(camera0.pos - point3D)\n",
    "        if dist < min_dist or dist > max_dist:\n",
    "            return\n",
    "        \n",
    "        #x, y = mvs.project_point(camera0, point3D)\n",
    "        #color = image0[helpers.round_to_int(y), helpers.round_to_int(x)]\n",
    "        \n",
    "        color = (1,0,0) # RED\n",
    "        event_queue.put((point3D, color))\n",
    "        redraw(pixel0, pixel1)\n",
    "    \n",
    "    # Call draw_epipolar_line (discarding output) so that it is compiled before the user starts clicking.\n",
    "    print(\"Compiling...\")\n",
    "    _ = draw_epipolar_photo_consistency(image0, image1, camera0, camera1, (150, 150), pc_func, support_domain)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    redraw()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", onclick)\n",
    "    #fig.show()\n",
    "    \n",
    "def plot_interactive_epipolar_photo_consistency(image0, image1, camera0, camera1, pc_func, support_domain, min_dist, max_dist):\n",
    "    event_queue = queue.Queue()\n",
    "    plot_epipolar_line_images2D(event_queue, image0, image1, camera0, camera1, pc_func, support_domain, min_dist, max_dist)\n",
    "    return plot_scatter3D(event_queue)\n",
    "\n",
    "fig = plot_interactive_epipolar_photo_consistency(\n",
    "    image0_color, image1_color,\n",
    "    camera0, camera1,\n",
    "    photo_consistency_SSD, 3,\n",
    "    min_dist, max_dist) # The images do not capture points that are this far away (any triangulated point that is is incorrect)\n",
    "# TODO: (exercise) click on image and draw epipolar line in the other image. 3D view next to it visualizing the 3D points while they are added\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b7e70f842ce4efcb80fd5811ef41507",
     "grade": false,
     "grade_id": "cell-50d2c99b9d13c901",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Complete 3D reconstruction\n",
    "Clicking on each pixel individually to compute a full 3D reconstruction will take a long time. So the following code will automatically perform the 3D reconstruction process for each pixel in the image. To reduce the potential error, triangulated points that lie very far away are considered false matches and are discarded. Furtermore, pixels for which there are multiple large photo consistency peaks are discarded.\n",
    "\n",
    "We visualize one out of every 30 found points to prevent your computer from slowing down to much.\n",
    "\n",
    "**NOTE** If you get a `ZeroDivisionError` then make sure that in `closest_points_on_rays` (exercise 1) you use `np.divide` instead of the `/` symbol (`np.divide` will not throw an exception when a division by 0 is encountered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04ee9f3b715232d317ce3302670cb599",
     "grade": false,
     "grade_id": "cell-2472fe6ef2be2cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import parallel\n",
    "\n",
    "@njit\n",
    "def local_maxima(signal):\n",
    "    result = []\n",
    "    for i in range(1, len(signal)-1):\n",
    "        if signal[i] > signal[i-1] and signal[i] > signal[i+1]:\n",
    "            result.append(signal[i])\n",
    "    return result\n",
    "\n",
    "@njit\n",
    "def normalize(signal):\n",
    "    min_signal = np.min(signal)\n",
    "    return (signal - min_signal) / (np.max(signal) - min_signal)\n",
    "\n",
    "def compute_point_cloud(image0, image1, camera0, camera1, pc_func, support_domain, min_dist, max_dist):\n",
    "    assert(image0.shape == image1.shape)\n",
    "    height, width = image0.shape[:2]\n",
    "       \n",
    "    @njit\n",
    "    def kernel(x, y):\n",
    "        if x < support_domain or x >= width - support_domain or y < support_domain or y >= height - support_domain:\n",
    "            return None\n",
    "        \n",
    "        pixel0 = (x, y)\n",
    "        pixel1s, photo_consistencies = mvs.compute_photo_consistency_along_ray_color_avg(image0, image1, camera0, camera1, pixel0, pc_func, support_domain)\n",
    "        if len(photo_consistencies) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Discard if there are multiple large peaks in the photo consistency.\n",
    "        normalized_photo_consistencies = normalize(photo_consistencies)\n",
    "        photo_consistency_peaks = local_maxima(normalized_photo_consistencies)\n",
    "        # https://stackoverflow.com/questions/33181350/quickest-way-to-find-the-nth-largest-value-in-a-numpy-matrix\n",
    "        second_largest_photo_consistency_peak = np.partition(photo_consistency_peaks, -2)[-2]\n",
    "        # Max peak = 1.0 (normalized) so if the second highest peak is higher than 0.975 the max peak then discard\n",
    "        if second_largest_photo_consistency_peak > 0.975:\n",
    "            return None\n",
    "        \n",
    "        pixel1 = pixel1s[np.argmax(photo_consistencies)]\n",
    "        ray0 = mvs.pixel_to_ray(camera0, pixel0)\n",
    "        ray1 = mvs.pixel_to_ray(camera1, pixel1)\n",
    "        point3D = closest_point_between_rays(ray0, ray1)\n",
    "\n",
    "        # Discard points that lie behind the cameras\n",
    "        if np.dot(point3D - camera0.pos, ray0[1]) < 0 or np.dot(point3D - camera1.pos, ray1[1]) < 0:\n",
    "            return None\n",
    "        \n",
    "        # Discard obviously wrong rays (behind the camera or very far away)\n",
    "        dist = np.linalg.norm(point3D - camera0.pos)\n",
    "        if dist < min_dist or dist > max_dist:\n",
    "            return None\n",
    "\n",
    "        return (point3D, image0[y, x])\n",
    "        \n",
    "    points3D = parallel.transform_reduce_2D_to_1D(kernel, width, height)\n",
    "    return (np.array([p for p, c in points3D], np.float32), np.array([c for p, c in points3D], np.float32))\n",
    "    \n",
    "import os\n",
    "if not os.environ.get(\"NBGRADER_VALIDATING\"): # Skip this code when grading\n",
    "    points3D, colors3D = compute_point_cloud(image1_color_quarter_res, image0_color_quarter_res, camera1_quarter_res, camera0_quarter_res,\n",
    "                                         photo_consistency_SSD, 3, min_dist, max_dist)\n",
    "\n",
    "    print(f\"Num points: {len(points3D)}\")\n",
    "    np.random.shuffle(points3D)\n",
    "    trace = plotly_points(points3D[::20,:], color=colors3D) # Only show one in every twenty points to improve performance\n",
    "    go.Figure(data=[trace], layout={\"scene\": {\"aspectmode\": \"cube\"}}).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
